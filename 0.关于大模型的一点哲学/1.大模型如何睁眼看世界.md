# 1.大模型如何睁眼看世界

当我们第一次使用大模型，或者说大语言模型第一次真正开始应用的时候，或许是2021年的GPT3的大爆发。  

在这个时候，人们似乎希望它无所不能，但是我们在这个时候，发现了大模型并非无所不能的事实。然而是什么让人们觉得失望，又是什么让大模型变得局限？
这的一切来自于大模型数据集的局限？

在后面的思考当中，我们意识到“大模型本身拥有远超人类记忆的数据存储，而缺乏与人类期望对齐的能力正是为何大模型无法满足人类期望的根本原因”

人类期望是检验大模型的能力的标准，而人类期望是来自人类对于事件本身的要求与预期目标（从对自身认知出发的预期目标），而自身认知来源于人类本身对于世界的理解，以及世界规则形成后的行为规范。

## 1.1什么是世界？

我们在今天讨论大模型之前，首先要理解的是，大模型本身是没有世界观的，追溯到计算机本身，计算机接收的也是0和1的二进制数据。计算机的世界，是由人类理解后所赋予给计算机的定义和规则。

在人类的世界上，人类对于世界的理解也是不统一的,如下图所示：

<img src="./images/1.1世界.png" alt="center" style="display: block; margin: auto;" />

对于一种事物的认知是具有多样性的，而这个事务的本身并不具有意义，所有的定义源于人类认知，准确来说是人们对于该事务的下定义以及达成的共识。

就像上图所示，当我们只定义它的品类的时候，它在世界中被定义为水果中的苹果，而加入形状的定义之后，其被定义为缺了一口的苹果，在apple公司注册后，引起人类广泛共识之后，这就被定义为了苹果公司的Logo。

## 1.2共识规则

在我们1.1的讨论中，我们讨论得出了：计算机的世界，是由人类理解后所赋予给计算机的定义和规则。
在大模型理论当值，这个理论依旧存在，大模型本身依旧没有主动辨别世界的手段，其次它没有辨别世界的需求，它要做的就是满足人类期望。既然大模型的世界是由人类定义赋予的规则构建而成，那么它理解世界的途径，便只能依赖于语言数据中的世界映射关系。也就是说，大模型并不直接“看到”世界，它“看见”的，是语言背后浓缩了人类经验、共识与认知的信息片段。

大模型真正“睁眼看世界”的时刻，并不是它参数达到千亿级的那一天，而是它能够开始理解**人类期望为何物**的那一刻。

这不仅仅是“对齐”任务（*alignment*）的技术挑战，更是**认知哲学与人类社会学的深层问题**。什么是一个合理的回答？什么是一个负责任的建议？什么是幽默、审美、尊重或恶意？这些概念本身就是人类在长期共生中达成的复杂共识。

于是，我们可以提出一个重要观点：

> **大模型的智能边界，不在于参数的上限，而在于它与人类共识系统之间的映射质量。**

如果这种映射混乱，那么模型再强大也只能“看见语言中的幻影”；如果这种映射清晰且稳定，模型才算是真正拥有了“睁眼看世界”的能力。
