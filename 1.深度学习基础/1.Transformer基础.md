# Transformer基础

## 1.Tansformer的论文以及贡献

首先应该提到的是Transformer的论文地址，以及提到Transform的重大意义

Transformer架构最初由Vaswani等人在2017年提出，论文标题为 **《Attention is All You Need》**，该论文彻底改变了自然语言处理（NLP）领域。该架构不依赖于传统的循环神经网络（RNN）或长短时记忆网络（LSTM），而是完全基于**自注意力机制**（Self-Attention），这使得模型在处理长距离依赖关系时更高效且能够并行计算。
论文地址：[Attention is All You Need](https://arxiv.org/abs/1706.03762)

Transformer架构，彻底颠覆了我们对“如何处理序列数据”的传统认知。想象一下，以前的RNN和LSTM像是跑步机上的慢跑选手，得一步一步慢慢来，而Transformer则是极速跑车，直接用“自注意力”机制让所有信息同时关注起来，飞速处理，分分钟并行计算，效率爆表！它不仅让长距离依赖问题迎刃而解，还让NLP任务像翻译、文本生成、情感分析等都迎来了“飞跃式”的进化。简而言之，Transformer就像是给机器翻译装上了“超能力”，然后不仅仅停留在NLP领域，还把这股“光速革命”带到了计算机视觉和语音处理等其他领域，成为了深度学习的“宇宙之王”。

| 特性                 | RNN                               | LSTM                              | Transformer                        |
|----------------------|-----------------------------------|-----------------------------------|------------------------------------|
| **计算方式**         | 顺序计算，逐步处理数据           | 顺序计算，但具有门控机制         | 并行计算，通过自注意力机制处理   |
| **长程依赖**         | 对长程依赖建模较差，容易丢失信息 | 能够通过门控机制记住长程依赖信息 | 通过自注意力机制轻松捕捉长程依赖 |
| **训练效率**         | 训练速度较慢，无法并行化         | 训练速度慢，虽然有所改进但仍需顺序处理 | 训练速度极快，支持高效并行计算   |
| **梯度问题**         | 容易出现梯度消失或梯度爆炸问题   | 通过门控机制解决了部分梯度问题   | 无梯度消失问题，完全避免了梯度问题 |
| **模型复杂度**       | 模型较为简单，计算资源消耗少     | 复杂的门控机制使得模型较为庞大   | 复杂度高，尤其是多头注意力部分   |
| **捕捉信息的方式**   | 逐步传递信息，每个步骤依赖前一步 | 通过门控机制精确控制信息流动     | 通过全局自注意力机制一次性获取所有信息 |
| **并行性**           | 不支持并行计算                   | 不支持并行计算                   | 强大的并行计算能力                |
| **主要应用**         | 序列数据（如时间序列预测）       | 机器翻译、语言建模、语音识别等   | 机器翻译、文本生成、BERT、GPT等 |
| **优点**             | 模型较简单，适用于小规模问题     | 可以记住长程依赖，适合复杂问题   | 高效并行、能处理长程依赖，广泛应用 |
| **缺点**             | 难以捕捉长程依赖，训练效率低     | 模型复杂，训练仍然较慢           | 计算量大，训练和推理资源消耗高   |

## 2.TransFormer原理

我们首先来看看Transformer的整体结构：

<img src="./images/Transformer.png" alt="center" style="display: block; margin: auto;" />

Transformer由Encoded和Decoder组成（编码器和解码器）构成。图片从中间分开，左边是编码器，右边是解码器

编码器由自注意力机制和前向传播神经网络组成，解码器的结构和编码器类似，但它额外增加了一个掩码注意力层(Masked Multi-Head  Attention)。

> **在我们统一来看的情况下：Transformer主要由 **自注意力机制**，**位置编码**， **前馈神经网络**进行组成**
> 
> 所以接下来我们就从：什么是注意力机制，位置编码进行展开。


### 2.1 自注意力机制

自注意力机制是Transformer架构的核心，它允许模型在处理每个词时，同时关注输入序列中的所有其他词。这种机制的本质是“给每个词一个注意力分数”，通过这些分数决定每个词对其他词的关注程度。

> 如果想要看一些幽默的讨论解释，可以参考[大模型你小子又在看哪](../0.关于大模型的一点哲学/2.大模型你小子又在看哪.md)




