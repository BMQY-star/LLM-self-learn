# Transformer基础

## 1.Tansformer的论文以及贡献

首先应该提到的是Transformer的论文地址，以及提到Transform的重大意义

Transformer架构最初由Vaswani等人在2017年提出，论文标题为 **《Attention is All You Need》**，该论文彻底改变了自然语言处理（NLP）领域。该架构不依赖于传统的循环神经网络（RNN）或长短时记忆网络（LSTM），而是完全基于**自注意力机制**（Self-Attention），这使得模型在处理长距离依赖关系时更高效且能够并行计算。
论文地址：[Attention is All You Need](https://arxiv.org/abs/1706.03762)

Transformer架构，彻底颠覆了我们对“如何处理序列数据”的传统认知。想象一下，以前的RNN和LSTM像是跑步机上的慢跑选手，得一步一步慢慢来，而Transformer则是极速跑车，直接用“自注意力”机制让所有信息同时关注起来，飞速处理，分分钟并行计算，效率爆表！它不仅让长距离依赖问题迎刃而解，还让NLP任务像翻译、文本生成、情感分析等都迎来了“飞跃式”的进化。简而言之，Transformer就像是给机器翻译装上了“超能力”，然后不仅仅停留在NLP领域，还把这股“光速革命”带到了计算机视觉和语音处理等其他领域，成为了深度学习的“宇宙之王”。

| 特性                 | RNN                               | LSTM                              | Transformer                        |
|----------------------|-----------------------------------|-----------------------------------|------------------------------------|
| **计算方式**         | 顺序计算，逐步处理数据           | 顺序计算，但具有门控机制         | 并行计算，通过自注意力机制处理   |
| **长程依赖**         | 对长程依赖建模较差，容易丢失信息 | 能够通过门控机制记住长程依赖信息 | 通过自注意力机制轻松捕捉长程依赖 |
| **训练效率**         | 训练速度较慢，无法并行化         | 训练速度慢，虽然有所改进但仍需顺序处理 | 训练速度极快，支持高效并行计算   |
| **梯度问题**         | 容易出现梯度消失或梯度爆炸问题   | 通过门控机制解决了部分梯度问题   | 无梯度消失问题，完全避免了梯度问题 |
| **模型复杂度**       | 模型较为简单，计算资源消耗少     | 复杂的门控机制使得模型较为庞大   | 复杂度高，尤其是多头注意力部分   |
| **捕捉信息的方式**   | 逐步传递信息，每个步骤依赖前一步 | 通过门控机制精确控制信息流动     | 通过全局自注意力机制一次性获取所有信息 |
| **并行性**           | 不支持并行计算                   | 不支持并行计算                   | 强大的并行计算能力                |
| **主要应用**         | 序列数据（如时间序列预测）       | 机器翻译、语言建模、语音识别等   | 机器翻译、文本生成、BERT、GPT等 |
| **优点**             | 模型较简单，适用于小规模问题     | 可以记住长程依赖，适合复杂问题   | 高效并行、能处理长程依赖，广泛应用 |
| **缺点**             | 难以捕捉长程依赖，训练效率低     | 模型复杂，训练仍然较慢           | 计算量大，训练和推理资源消耗高   |

## 2.TransFormer原理

我们首先来看看Transformer的整体结构：

<img src="./images/Transformer.png" alt="center" style="display: block; margin: auto;" />

Transformer由Encoded和Decoder组成（编码器和解码器）构成。图片从中间分开，左边是编码器，右边是解码器

编码器由自注意力机制和前向传播神经网络组成，解码器的结构和编码器类似，但它额外增加了一个掩码注意力层(Masked Multi-Head  Attention)。

**在我们统一来看的情况下：Transformer主要由 **自注意力机制**，**位置编码**， **前馈神经网络**进行组成**

接下来我们就从：词向量，位置编码，自注意力机制进行展开

### 2.2 Tokenizer与词向量

在 Transformer 模型中，模型无法直接处理原始文本，必须先将其转换为数值形式。本节介绍这个转换的两个关键步骤：**Tokenizer（分词器）** 和 **词向量（Word Embedding）**  
**Tokenizer** 是文本输入模型之前的第一步，作用是：

> 将一句自然语言切分为若干个 token（符号），并将这些 token 映射为数字 ID。

简单来说，Tokenizer 的作用是将文本中的词语转换为唯一的 ID，便于模型进行处理。

在早期的研究中，学者们曾尝试使用 one-hot 独热编码来表示每个词。然而，one-hot 编码会为每个词创建一个与词表大小相同的高维向量，只有其中一个位置为 1，其余全为 0。

例如，当词表中有 1 亿个词时，每个词的表示都是一个 1 亿维的稀疏向量。这样的表示方式不仅占用大量内存，而且在实际计算中效率极低，大大增加了模型的计算负担。

因此，现代 NLP 模型普遍使用 Tokenizer 将词语映射为紧凑的 ID，然后再通过词嵌入（embedding）将其转化为低维、密集的向量，既节省资源又提升效率。

>示例：  
> 原始句子：我是一名学生。  
> 经过 tokenizer 处理后可能变为：["我", "是", "一名", "学生", "。"]  
> 然后根据词表（vocab）映射为数字 ID：[101, 3221, 2769, 738, 102]
> - 每个 ID 对应词表中的一个词或子词
> - 有些 tokenizer（如 BERT 的 WordPiece）甚至会将词拆成更细的子词单位

词向量（Word Embedding）：让模型“理解”词语

有了 token ID 后，下一步是用 **词向量** 将这些离散的数字转换为模型可以计算的稠密向量。

> 每个 token ID 会被映射成一个向量，比如 512 维的浮点数数组。

这些 ID 就可以输入到模型的下一步：词嵌入层。


有了 token ID 后，模型并不能直接“理解”这些数字的语义。为了让模型具备语义感知能力，需要通过词向量将这些 ID 映射为稠密的实数向量。

每个 token ID 会被映射成一个低维向量，比如 512 维或 768 维的浮点向量，这些向量是模型训练过程中自动学习出来的。

Token ID 只是一个整数编号，本身不含任何语义信息。比如：

>“我” 可能是 ID 1001   
>“你” 可能是 ID 1002

这两个数字之间没有任何“我”和“你”的语义关系，模型也无法比较它们的相似性。词嵌入就是解决这个问题的关键。

**嵌入向量的特点**：  
>低维、连续：通常是 128、256、512 或 768 维浮点数组

>可学习：词向量在训练中不断调整，逐步捕捉词之间的语义差异与关联

>表达丰富：相近语义的词在向量空间中的距离更接近

示例：

| 词语 | Token ID | One-Hot 向量         | Embedding 向量（示意）             |
| -- | -------- | ------------------ | ---------------------------- |
| 我  | 1001     | \[0, 0, ..., 1, 0] | \[0.8, 0.4, -0.1, ..., 0.05] |
| 学生 | 1005     | \[0, 0, ..., 1, 0] | \[0.7, 0.5, 0.2, ..., -0.03] |


### 2.3 位置编码

| 特性       | RNN                   | Transformer  |
| -------- | --------------------- | ------------ |
| 输入方式     | 串行，一个词接一个词            | 并行，一次处理整个序列  |
| 顺序建模方式   | 隐含在 hidden state 的传递中 | 通过位置编码显式加入   |
| 是否需要位置编码 | 不需要                   | 必须有，否则无“位置感” |

RNN（循环神经网络） 是按顺序逐词处理的，每次输入一个词：

>x₁ → h₁ → x₂ → h₂ → x₃ → h₃ → ...
>由于这种“串行处理”的结构，顺序信息是天然包含在状态传递（hidden state）中的，所以不需要额外加“位置编码”。

而 Transformer 是一种并行架构，它一次性输入整个序列（如整句话）进行处理：

>[x₁, x₂, x₃, ..., xₙ] → 同时送入多头注意力层
>在这种结构中，模型只看到了一堆词向量，它无法区分“我 是 学生” 和 “学生 是 我”，因为没有顺序概念。

所以必须通过 位置编码（Positional Encoding） 显式告诉模型每个词是第几个。

位置编码的计算方式如下：

>$$
\text{PE}_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)   
$$
>$$
\text{PE}_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)
$$
>其中：
>- $pos$ 表示词在句子中的位置（从 0 开始）
>- $i$ 表示词向量的第 $i$ 个维度
>- $d$ 是词向量的总维度（如 512、768）

### 2.4 自注意力机制

自注意力机制是Transformer架构的核心，它允许模型在处理每个词时，同时关注输入序列中的所有其他词。这种机制的本质是“给每个词一个注意力分数”，通过这些分数决定每个词对其他词的关注程度。

> 如果想要看一些幽默的讨论解释，可以参考[大模型你小子又在看哪](../0.关于大模型的一点哲学/2.大模型你小子又在看哪.md)

> **本节的代码实现在[1.Transformer基础](1.transformer基础.ipynb)中的第5点**

我们首先看到的就是自注意力机制的公式


$$
\text{Attention}(K, Q, V) = \text{Softmax}\left( \frac{K^\top Q}{\sqrt{d_k}} \right) V
$$

在这个公式中，K、Q 和 V 是从输入向量中通过线性变换生成的三组不同表示。我们可以通过一个具体句子来帮助理解，比如：

> 句子：“我 是 一名 学生”

当模型处理“学生”这个词时，它会进行如下操作：

- **Q（Query）**：由“学生”生成，表示“学生”当前想要寻找与它相关的信息；
- **K（Key）**：由整句中所有词（包括“我”、“是”、“一名”、“学生”）生成，表示每个词的“身份特征”；
- **V（Value）**：同样由每个词生成，表示它们携带的实际语义信息。

“学生”作为 Query，会依次与“我”、“是”、“一名”、“学生”这几个词的 Key 做匹配（点积），计算出每个词对它的重要性（注意力分数）。然后根据这些分数去加权整句话中所有词的 Value，从而得到一个融合了上下文语义的“学生”的新表示。

这个机制允许模型在表示“学生”时，自动地“注意”到“我”是主语，“一名”是修饰词，从而更好地理解整句话的含义。


Transformer 不仅使用了 **自注意力（Self-Attention）**，也使用了 **互注意力（Cross-Attention）**。

- 在 **自注意力** 中，图中的 K、Q 和 V 都来自输入 `X`，也就是说：
  - `Q = X · W_Q`
  - `K = X · W_K`
  - `V = X · W_V`

- 在 **互注意力** 中，K 和 V 来自编码器的输出，而 Q 来自译码器的输入：
  - `Q = DecoderInput · W_Q`
  - `K = EncoderOutput · W_K`
  - `V = EncoderOutput · W_V`

### 可视化：注意力矩阵

自注意力机制的本质，是为每个词生成一个“关注其他词”的权重向量。这些权重构成了一个注意力矩阵（Attention Matrix），可以用图像形式表示出来。

> 行表示当前处理的词，列表示它关注其他词的程度。

例如：

|      | 我 | 是 | 一名 | 学生 |
|------|----|----|------|------|
| 我   | 0.2 | 0.3 | 0.1  | 0.4  |
| 是   | 0.1 | 0.6 | 0.2  | 0.1  |
| 一名 | 0.05| 0.1 | 0.7  | 0.15 |
| 学生 | 0.3 | 0.2 | 0.1  | 0.4  |



### 小结

- 自注意力机制让每个词在表示自身时，可以结合全局上下文；
- Q、K、V 是通过线性变换生成的；
- 注意力矩阵通过点积 + softmax 得到，控制每个词对其他词的关注；
- 自注意力是 Transformer 编码器的核心；互注意力是解码器的关键。






